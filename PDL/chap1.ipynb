{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Transfer Learning (VGG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 ImageNet Dataset & VGG-16 Model\n",
    "\n",
    "### 1) ImageNet Dataset\n",
    "\n",
    "ImageNet Dataset은 MNIST, CIFAR 데이터셋과 더불어 굉장히 유명한 데이터셋이다. 다른 두 개의 데이터셋은 굉장히 작은 데이터셋으로 아이디어 검증에 사용된다.\n",
    "\n",
    "* MNIST: 28x28 손글씨 사진 (학습 데이터 6만 장, 테스트 데이터 1만 장)  \n",
    "    ![출처: https://www.google.com/url?sa=i&url=https%3A%2F%2Fko.wikipedia.org%2Fwiki%2FMNIST_%25EB%258D%25B0%25EC%259D%25B4%25ED%2584%25B0%25EB%25B2%25A0%25EC%259D%25B4%25EC%258A%25A4&psig=AOvVaw05FlDMkezOEfvgGw6PkpA6&ust=1629965252703000&source=images&cd=vfe&ved=0CAoQjRxqFwoTCJDVo_fby_ICFQAAAAAdAAAAABAD](images/MnistExamples.png)\n",
    "\n",
    "* CIFAR-10: 10개의 클래스로 구분된 32x32 사물 사진 (학습 데이터 5만 장, 테스트 데이터 1만 장)  \n",
    "    ![출처: https://www.google.com/url?sa=i&url=https%3A%2F%2Fgithub.com%2Fdnddnjs%2Fpytorch-cifar10&psig=AOvVaw0s2ba3rx9gg6a7iOhwcexL&ust=1629965198624000&source=images&cd=vfe&ved=0CAoQjRxqFwoTCNjb593by_ICFQAAAAAdAAAAABAV](images/cifar-10.jpg)\n",
    "\n",
    "반면, ImageNet 데이터셋은 대표적인 대규모 데이터셋이다. 전체 데이터셋에 포함된 이미지는 1,000만 장이 넘으며 이는 Amazon Mechanical Turk 서비스를 이용하여 일일이 사람이 분류한 데이터셋이다. 이에 대한 간략한 설명은 다음과 같다.\n",
    "\n",
    "* ImageNet Dataset: 스탠포드 대학교에서 인터넷 화상을 수집해 분류한 데이터셋으로 ILSVRC(ImageNet Large Scale Visual Recognition Challenge)대회에서 사용  \n",
    "    ![출처: https://www.google.com/url?sa=i&url=https%3A%2F%2Fdevopedia.org%2Fimagenet&psig=AOvVaw0Id-a6a10d8h0KDF4m-f1A&ust=1629965635671000&source=images&cd=vfe&ved=0CAoQjRxqFwoTCOjOv6ndy_ICFQAAAAAdAAAAABAD](images/imagenet.png)\n",
    "\n",
    "특히 PyTorch는 ImageNet 데이터셋 중 ILSVRC2012 데이터셋(클래스: 1천 개, 학습 데이터: 120만 장, 검증 데이터: 5만 장, 테스트 데이터: 10만 장)으로 신경망의 parameter를 학습한 다양한 모델을 사용할 수 있다.\n",
    "\n",
    "### 2) VGG-16 Model\n",
    "\n",
    "VGG-16 모델은 2014년 ILSVRC에서 2위를 차지한 합성곱 신경망이다. (비록 우승은 GoogleNet이었지만, 구조의 간결함과 사용의 편이성으로 인해 VGG-16이 GoogleNet보다 더 각광받았다.) 옥스포드 대학교의 VGG팀이 제작하였으며 16층으로 구성되었기때문에 VGG-16으로 불린다. 11, 13, 19층 버전의 모델도 존재하며 구성이 간단하기 때문에 다양한 딥러닝 응용 기술의 기반 네트워크로 사용한다. \n",
    "\n",
    "![출처: https://bskyvision.com/504](images/vggnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Prepare Projects\n",
    "\n",
    "### 1) Make Directories\n",
    "\n",
    "```sh\n",
    "mkdir data\n",
    "mkdir utils\n",
    "```\n",
    "\n",
    "### 2) Download Image\n",
    "\n",
    "```sh\n",
    "cd data\n",
    "wget https://cdn.pixabay.com/photo/2018/10/05/02/26/goldenretriever-3724972_960_720.jpg\n",
    "```\n",
    "\n",
    "### 3) Install Package (For Arch based Distros)\n",
    "\n",
    "```sh\n",
    "# Install PyTorch (SIMD)\n",
    "yay -S python-pytorch-opt\n",
    "yay -S python-pillow-simd\n",
    "yay -S libpng\n",
    "\n",
    "# Install torchvision\n",
    "git clone https://github.com/pytorch/vision\n",
    "cd vision\n",
    "python setup.py install --user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.9.0\n",
      "Torchvision Version:  0.11.0a0+b72129c\n"
     ]
    }
   ],
   "source": [
    "# Check Versions\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.5 Read VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/xteca/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd535858881944ec8b3c75430a255d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of VGG-16\n",
    "net = models.vgg16(pretrained=True)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VGG-16의 네트워크 구성은 `features`와 `classifier` 두 모듈로 구성되어 있으며, 각 모듈 속에 합성곱(Convolution) 층과 전결합(Linear) 층이 있다. \n",
    "    ![출처: https://neurohive.io/en/popular-networks/vgg16/](images/vgg16.png)\n",
    "    \n",
    "  \n",
    "* VGG-16이 16계층이란 것은 Activation(ReLU), Pooling, Dropout을 제외한 Convolution layer와 Linear Layer 수를 말하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch summary (`pip install torchsummary`)\n",
    "from torchsummary import summary as summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary_(net, input_size=(3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 번외 - Convolution & Pooling\n",
    "\n",
    "Convolution은 이미지에서 Feature를 추출할 때 사용되는 방법이다. 일반적으로 이미지보다 작은 Kernel을 가지고 이미지를 순회하면서 내적하여 구한 Scalar 값을 모아 이미지를 축소한다.\n",
    "\n",
    "![출처: Deep Learning with PyTorch](images/convolution.png)\n",
    "\n",
    "![출처: https://gaussian37.github.io/dl-concept-covolution_operation/](images/conv.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* 안경잡이개발자, [ImageNet 데이터셋 소개 및 다운로드하는 방법](https://ndb796.tistory.com/471)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
